{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RigEhsbQfGvs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "import wandb\n",
        "\n",
        "# Load the dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "# Normalize images\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "# Define class names\n",
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
        "#intialize wanddb\n",
        "wandb.init(project=\"deep_learning\",name='mnist_dataset')\n",
        "# Select one image per class\n",
        "samples_per_class = {}\n",
        "for img, label in zip(train_images, train_labels):\n",
        "    if label not in samples_per_class:\n",
        "        samples_per_class[label] = img\n",
        "    if len(samples_per_class) == len(class_names):\n",
        "        break\n",
        "wandb.log({\"Fashion-MNIST Sample Images\": [wandb.Image(img, caption=class_names[label]) for label, img in samples_per_class.items()]})\n",
        "\n",
        "# Plot images\n",
        "fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
        "fig.suptitle(\"Fashion-MNIST Sample Images\", fontsize=14)\n",
        "\n",
        "for ax, (label, image) in zip(axes.flat, samples_per_class.items()):\n",
        "    ax.imshow(image, cmap='gray')\n",
        "    ax.set_title(class_names[label])\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#flatten the images\n",
        "train_images = train_images.reshape(train_images.shape[0], -1)\n",
        "test_images = test_images.reshape(test_images.shape[0], -1)"
      ],
      "metadata": {
        "id": "rTX0sCwdgbH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Helper Functions\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Subtract max for numerical stability\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_derivative(z):\n",
        "    return (z > 0).astype(float)\n",
        "\n",
        "\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    n_samples = y_true.shape[0]\n",
        "    logp = -np.log(y_pred[range(n_samples), y_true])\n",
        "    return np.sum(logp) / n_samples\n",
        "# Neural Network Class\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, layer_sizes):\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        self._initialize_parameters()\n",
        "\n",
        "    def _initialize_parameters(self):\n",
        "        \"\"\"Initialize weights and biases for all layers.\"\"\"\n",
        "        for i in range(len(self.layer_sizes) - 1):\n",
        "            fan_in = self.layer_sizes[i]\n",
        "            fan_out = self.layer_sizes[i + 1]\n",
        "            # He initialization for hidden layers, Xavier for output layer\n",
        "            if i < len(self.layer_sizes) - 2:  # Hidden layers use sigmoid\n",
        "                scale = np.sqrt(2 / fan_in)\n",
        "            else:  # Output layer uses softmax\n",
        "                scale = np.sqrt(1 / fan_in)\n",
        "            # Weight matrix: (fan_in, fan_out)\n",
        "            weight = np.random.randn(fan_in, fan_out) * scale\n",
        "            # Bias vector: (fan_out,)\n",
        "            bias = np.zeros(fan_out)\n",
        "            self.weights.append(weight)\n",
        "            self.biases.append(bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        a = x  # Activation starts as the input\n",
        "        # Process hidden layers\n",
        "        for l in range(len(self.weights) - 1):\n",
        "            z = np.dot(a, self.weights[l]) + self.biases[l]\n",
        "            a = sigmoid(z)\n",
        "        # Process output layer\n",
        "        z = np.dot(a, self.weights[-1]) + self.biases[-1]\n",
        "        output = softmax(z)\n",
        "        return output\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Create a network: 784 input -> 256 hidden -> 128 hidden -> 10 output\n",
        "    nn = NeuralNetwork([784, 256, 128, 10])\n",
        "    # Simulate 5 input samples (e.g., Fashion-MNIST images)\n",
        "    x = np.random.randn(5, 784)\n",
        "    # Get probabilities\n",
        "    probs = nn.forward(x)\n",
        "    # Print probabilities for each sample\n",
        "    for i, prob in enumerate(probs):\n",
        "        print(f\"Sample {i+1} probabilities: {prob}\")"
      ],
      "metadata": {
        "id": "7d23rW0dgoX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZxGpz98KhUAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSqUzepQiKKg"
      },
      "outputs": [],
      "source": [
        "def to_one_hot(labels, num_classes=10):\n",
        "    \"\"\"Convert labels to one-hot encoding.\"\"\"\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "# Base Optimizer Class\n",
        "class Optimizer:\n",
        "    \"\"\"Abstract base class for optimizers.\"\"\"\n",
        "    def update(self, weights, biases, grads_w, grads_b):\n",
        "        raise NotImplementedError(\"Subclasses must implement this method.\")\n",
        "\n",
        "# SGD Optimizer\n",
        "class SGD(Optimizer):\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def update(self, weights, biases, grads_w, grads_b):\n",
        "        for i in range(len(weights)):\n",
        "            weights[i] -= self.learning_rate * grads_w[i]\n",
        "            biases[i] -= self.learning_rate * grads_b[i]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIzCahFHjV1t"
      },
      "outputs": [],
      "source": [
        "# Momentum Optimizer\n",
        "class Momentum(Optimizer):\n",
        "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.momentum = momentum\n",
        "        self.velocity_w = None\n",
        "        self.velocity_b = None\n",
        "\n",
        "    def update(self, weights, biases, grads_w, grads_b):\n",
        "        if self.velocity_w is None:\n",
        "            self.velocity_w = [np.zeros_like(w) for w in weights]\n",
        "            self.velocity_b = [np.zeros_like(b) for b in biases]\n",
        "        for i in range(len(weights)):\n",
        "            self.velocity_w[i] = self.momentum * self.velocity_w[i] + self.learning_rate * grads_w[i]\n",
        "            self.velocity_b[i] = self.momentum * self.velocity_b[i] + self.learning_rate * grads_b[i]\n",
        "            weights[i] -= self.velocity_w[i]\n",
        "            biases[i] -= self.velocity_b[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZoZNb2PjdYq"
      },
      "outputs": [],
      "source": [
        "# Nesterov Optimizer\n",
        "class Nesterov(Optimizer):\n",
        "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.momentum = momentum\n",
        "        self.velocity_w = None\n",
        "        self.velocity_b = None\n",
        "\n",
        "    def update(self, weights, biases, grads_w, grads_b):\n",
        "        if self.velocity_w is None:\n",
        "            self.velocity_w = [np.zeros_like(w) for w in weights]\n",
        "            self.velocity_b = [np.zeros_like(b) for b in biases]\n",
        "        for i in range(len(weights)):\n",
        "            # Simplified Nesterov: update with current gradients (approximation)\n",
        "            self.velocity_w[i] = self.momentum * self.velocity_w[i] + self.learning_rate * grads_w[i]\n",
        "            self.velocity_b[i] = self.momentum * self.velocity_b[i] + self.learning_rate * grads_b[i]\n",
        "            weights[i] -= self.momentum * self.velocity_w[i]\n",
        "            biases[i] -= self.momentum * self.velocity_b[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alprFUnmjh4E"
      },
      "outputs": [],
      "source": [
        "# RMSProp Optimizer\n",
        "class RMSProp(Optimizer):\n",
        "    def __init__(self, learning_rate=0.001, beta=0.9, epsilon=1e-8):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta = beta\n",
        "        self.epsilon = epsilon\n",
        "        self.s_w = None\n",
        "        self.s_b = None\n",
        "\n",
        "    def update(self, weights, biases, grads_w, grads_b):\n",
        "        if self.s_w is None:\n",
        "            self.s_w = [np.zeros_like(w) for w in weights]\n",
        "            self.s_b = [np.zeros_like(b) for b in biases]\n",
        "        for i in range(len(weights)):\n",
        "            self.s_w[i] = self.beta * self.s_w[i] + (1 - self.beta) * (grads_w[i] ** 2)\n",
        "            self.s_b[i] = self.beta * self.s_b[i] + (1 - self.beta) * (grads_b[i] ** 2)\n",
        "            weights[i] -= self.learning_rate * grads_w[i] / (np.sqrt(self.s_w[i] + self.epsilon))\n",
        "            biases[i] -= self.learning_rate * grads_b[i] / (np.sqrt(self.s_b[i] + self.epsilon))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4Kn4kYLjo82"
      },
      "outputs": [],
      "source": [
        "# Adam Optimizer\n",
        "class Adam(Optimizer):\n",
        "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.m_w = None\n",
        "        self.m_b = None\n",
        "        self.v_w = None\n",
        "        self.v_b = None\n",
        "        self.t = 0\n",
        "\n",
        "    def update(self, weights, biases, grads_w, grads_b):\n",
        "        if self.m_w is None:\n",
        "            self.m_w = [np.zeros_like(w) for w in weights]\n",
        "            self.m_b = [np.zeros_like(b) for b in biases]\n",
        "            self.v_w = [np.zeros_like(w) for w in weights]\n",
        "            self.v_b = [np.zeros_like(b) for b in biases]\n",
        "        self.t += 1\n",
        "        for i in range(len(weights)):\n",
        "            self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * grads_w[i]\n",
        "            self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * grads_b[i]\n",
        "            self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (grads_w[i] ** 2)\n",
        "            self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (grads_b[i] ** 2)\n",
        "            m_w_hat = self.m_w[i] / (1 - self.beta1 ** self.t)\n",
        "            m_b_hat = self.m_b[i] / (1 - self.beta1 ** self.t)\n",
        "            v_w_hat = self.v_w[i] / (1 - self.beta2 ** self.t)\n",
        "            v_b_hat = self.v_b[i] / (1 - self.beta2 ** self.t)\n",
        "            weights[i] -= self.learning_rate * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
        "            biases[i] -= self.learning_rate * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdENND-1jq6w"
      },
      "outputs": [],
      "source": [
        "# Nadam Optimizer\n",
        "class Nadam(Optimizer):\n",
        "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.m_w = None\n",
        "        self.m_b = None\n",
        "        self.v_w = None\n",
        "        self.v_b = None\n",
        "        self.t = 0\n",
        "\n",
        "    def update(self, weights, biases, grads_w, grads_b):\n",
        "        if self.m_w is None:\n",
        "            self.m_w = [np.zeros_like(w) for w in weights]\n",
        "            self.m_b = [np.zeros_like(b) for b in biases]\n",
        "            self.v_w = [np.zeros_like(w) for w in weights]\n",
        "            self.v_b = [np.zeros_like(b) for b in biases]\n",
        "        self.t += 1\n",
        "        for i in range(len(weights)):\n",
        "            self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * grads_w[i]\n",
        "            self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * grads_b[i]\n",
        "            self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (grads_w[i] ** 2)\n",
        "            self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (grads_b[i] ** 2)\n",
        "            m_w_hat = self.m_w[i] / (1 - self.beta1 ** self.t)\n",
        "            m_b_hat = self.m_b[i] / (1 - self.beta1 ** self.t)\n",
        "            v_w_hat = self.v_w[i] / (1 - self.beta2 ** self.t)\n",
        "            v_b_hat = self.v_b[i] / (1 - self.beta2 ** self.t)\n",
        "            m_w_nesterov = self.beta1 * m_w_hat + (1 - self.beta1) * grads_w[i] / (1 - self.beta1 ** self.t)\n",
        "            m_b_nesterov = self.beta1 * m_b_hat + (1 - self.beta1) * grads_b[i] / (1 - self.beta1 ** self.t)\n",
        "            weights[i] -= self.learning_rate * m_w_nesterov / (np.sqrt(v_w_hat) + self.epsilon)\n",
        "            biases[i] -= self.learning_rate * m_b_nesterov / (np.sqrt(v_b_hat) + self.epsilon)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import wandb\n",
        "from sklearn.model_selection import train_test_split\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    epsilon = 1e-15\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "    return -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]\n",
        "# Neural Network class\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, layer_sizes, activation='relu', weight_init='xavier'):\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.num_layers = len(layer_sizes)\n",
        "        self.activation = activation\n",
        "        self.weight_init = weight_init\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        for i in range(1, self.num_layers):\n",
        "            fan_in = layer_sizes[i-1]\n",
        "            fan_out = layer_sizes[i]\n",
        "            if weight_init == 'xavier':\n",
        "                weight = np.random.randn(fan_out, fan_in) * np.sqrt(2 / (fan_in + fan_out))\n",
        "            else:  # random\n",
        "                weight = np.random.randn(fan_out, fan_in) * 0.01\n",
        "            bias = np.zeros(fan_out)\n",
        "            self.weights.append(weight)\n",
        "            self.biases.append(bias)\n",
        "    def forward(self, X, training=False):\n",
        "        a = X\n",
        "        if training:\n",
        "            self.activations = [a]\n",
        "            self.zs = []\n",
        "        for l in range(self.num_layers - 1):\n",
        "            z = np.dot(a, self.weights[l].T) + self.biases[l]\n",
        "            if training:\n",
        "                self.zs.append(z)\n",
        "            if l < self.num_layers - 2:\n",
        "                if self.activation == 'relu':\n",
        "                    a = relu(z)\n",
        "                elif self.activation == 'sigmoid':\n",
        "                    a = sigmoid(z)\n",
        "                elif self.activation == 'tanh':\n",
        "                    a = np.tanh(z)\n",
        "            else:\n",
        "                a = softmax(z)\n",
        "            if training:\n",
        "                self.activations.append(a)\n",
        "        return a\n",
        "\n",
        "    def backward(self, y, weight_decay=0):\n",
        "        batch_size = y.shape[0]\n",
        "        grads_w = []\n",
        "        grads_b = []\n",
        "        dz = self.activations[-1] - y\n",
        "        for l in range(self.num_layers - 2, -1, -1):\n",
        "            a_prev = self.activations[l]\n",
        "            dW = np.dot(dz.T, a_prev) / batch_size + weight_decay * self.weights[l]\n",
        "            db = np.sum(dz, axis=0) / batch_size\n",
        "            grads_w.insert(0, dW)\n",
        "            grads_b.insert(0, db)\n",
        "            if l > 0:\n",
        "                z = self.zs[l-1]\n",
        "                if self.activation == 'relu':\n",
        "                    sigma_prime = (z > 0).astype(float)\n",
        "                elif self.activation == 'sigmoid':\n",
        "                    a = sigmoid(z)\n",
        "                    sigma_prime = a * (1 - a)\n",
        "                elif self.activation == 'tanh':\n",
        "                    a = np.tanh(z)\n",
        "                    sigma_prime = 1 - a**2\n",
        "                dz = np.dot(dz, self.weights[l]) * sigma_prime\n",
        "        return grads_w, grads_b\n",
        "\n",
        "    def train(self, X_train, y_train, X_val, y_val, epochs, batch_size, optimizer, weight_decay=0):\n",
        "        num_samples = X_train.shape[0]\n",
        "        y_train_onehot = to_one_hot(y_train, 10)\n",
        "        y_val_onehot = to_one_hot(y_val, 10)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training phase\n",
        "            train_loss = 0\n",
        "            indices = np.random.permutation(num_samples)\n",
        "            X_shuffled = X_train[indices]\n",
        "            y_shuffled = y_train_onehot[indices]\n",
        "            for i in range(0, num_samples, batch_size):\n",
        "                X_batch = X_shuffled[i:i + batch_size]\n",
        "                y_batch = y_shuffled[i:i + batch_size]\n",
        "                y_pred_batch = self.forward(X_batch, training=True)\n",
        "                train_loss += cross_entropy_loss(y_batch, y_pred_batch) * X_batch.shape[0]\n",
        "                grads_w, grads_b = self.backward(y_batch, weight_decay)\n",
        "                optimizer.update(self.weights, self.biases, grads_w, grads_b)\n",
        "            train_loss /= num_samples  # Average training loss per sample\n",
        "\n",
        "            # Compute training accuracy\n",
        "            train_acc = self.compute_accuracy(X_train, y_train)\n",
        "\n",
        "            # Validation phase\n",
        "            y_pred_val = self.forward(X_val, training=False)\n",
        "            val_loss = cross_entropy_loss(y_val_onehot, y_pred_val)\n",
        "            val_acc = self.compute_accuracy(X_val, y_val)\n",
        "\n",
        "            # Log metrics to Wandb\n",
        "            # wandb.log({\n",
        "            #     'epoch': epoch + 1,\n",
        "            #     'train_loss': train_loss,\n",
        "            #     'train_accuracy': train_acc,\n",
        "            #     'val_loss': val_loss,\n",
        "            #     'val_accuracy': val_acc\n",
        "            # })\n",
        "            # val_loss,val_acc,\n",
        "        return  grads_b, grads_w\n",
        "\n",
        "        # # Final metrics after training\n",
        "        # final_train_loss = cross_entropy_loss(y_train_onehot, self.forward(X_train, training=False))\n",
        "        # final_val_loss = val_loss\n",
        "        # final_val_acc = val_acc\n",
        "        # wandb.log({\n",
        "        #     'final_train_loss': final_train_loss,\n",
        "        #     'final_val_loss': final_val_loss,\n",
        "        #     'final_val_accuracy': final_val_acc\n",
        "        # })\n",
        "\n",
        "        # return final_val_acc, final_val_loss\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.forward(X, training=False)\n",
        "\n",
        "    def compute_accuracy(self, X, y):\n",
        "        predictions = np.argmax(self.forward(X, training=False), axis=1)\n",
        "        return np.mean(predictions == y)\n",
        "\n",
        "\n",
        "# # Train the network\n",
        "# x_train = train_images.reshape(train_images.shape[0], -1) / 255.0\n",
        "# x_test = test_images.reshape(test_images.shape[0], -1) / 255.0\n",
        "# nn = NeuralNetwork([784, 128, 10],'relu','random')\n",
        "# optimizer = SGD(learning_rate=0.001)\n",
        "# nn.train(x_train, train_labels, x_test, test_labels, epochs=5, batch_size=16, optimizer=optimizer)\n"
      ],
      "metadata": {
        "id": "oT5ki6VFhtoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wandb sweap configuration\n",
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n",
        "    'parameters': {\n",
        "        'epochs': {'values': [5, 10]},\n",
        "        'num_hidden_layers': {'values': [3, 4, 5]},\n",
        "        'hidden_size': {'values': [32, 64, 128]},\n",
        "        'weight_decay': {'values': [0, 0.0005, 0.5]},\n",
        "        'learning_rate': {'values': [1e-3, 1e-4]},\n",
        "        'optimizer': {'values': ['sgd', 'momentum', 'adam','nadam', 'rmsprop','nesterov' ]},\n",
        "        'batch_size': {'values': [16, 32, 64]},\n",
        "        'weight_init': {'values': ['random', 'xavier']},\n",
        "        'activation': {'values': ['sigmoid', 'tanh', 'relu']}\n",
        "    }\n",
        "}\n",
        "\n",
        "# Training function for Wandb sweep\n",
        "def train():\n",
        "    config_defaults = {\n",
        "        'epochs': 5,\n",
        "        'num_hidden_layers': 3,\n",
        "        'hidden_size': 32,\n",
        "        'weight_decay': 0,\n",
        "        'learning_rate': 1e-3,\n",
        "        'optimizer': 'adam',\n",
        "        'batch_size': 32,\n",
        "        'weight_init': 'xavier',\n",
        "        'activation': 'relu'\n",
        "    }\n",
        "    # Initialize Wandb run\n",
        "    run_name = f\"hl_{config_defaults['num_hidden_layers']}_bs_{config_defaults['batch_size']}_ac_{config_defaults['activation']}\"\n",
        "    wandb.init(config=config_defaults, name=run_name)\n",
        "    config = wandb.config\n",
        "\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(train_images, train_labels, test_size=0.1, random_state=42)\n",
        "\n",
        "    # Define network architecture\n",
        "    layer_sizes = [784] + [config.hidden_size] * config.num_hidden_layers + [10]\n",
        "    nn = NeuralNetwork(layer_sizes, activation=config.activation, weight_init=config.weight_init)\n",
        "\n",
        "    # Select optimizer\n",
        "    if config.optimizer == 'sgd':\n",
        "        optimizer = SGD(learning_rate=config.learning_rate)\n",
        "    elif config.optimizer == 'momentum':\n",
        "        optimizer = Momentum(learning_rate=config.learning_rate)\n",
        "    elif config.optimizer == 'adam':\n",
        "        optimizer = Adam(learning_rate=config.learning_rate)\n",
        "    elif config.optimizer == 'nadam':\n",
        "        optimizer = Nadam(learning_rate=config.learning_rate)\n",
        "    elif config.optimizer == 'rmsprop':\n",
        "        optimizer = RMSProp(learning_rate=config.learning_rate)\n",
        "    elif config.optimizer == 'nesterov':\n",
        "        optimizer = Nesterov(learning_rate=config.learning_rate)\n",
        "\n",
        "    # Train the model\n",
        "    val_acc, val_loss = nn.train(X_train, y_train, X_val, y_val, config.epochs, config.batch_size, optimizer, config.weight_decay)\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "# Initialize and run the sweep\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"deep_learning\")\n",
        "wandb.agent(sweep_id, function=train, count=50)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "mzq3nDm9h1lx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}