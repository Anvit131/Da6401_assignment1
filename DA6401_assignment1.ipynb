{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RigEhsbQfGvs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "import wandb\n",
        "\n",
        "# Load the dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "# Normalize images\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "# Define class names\n",
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
        "#intialize wanddb\n",
        "wandb.init(project=\"deep_learning\",name='mnist_dataset')\n",
        "# Select one image per class\n",
        "samples_per_class = {}\n",
        "for img, label in zip(train_images, train_labels):\n",
        "    if label not in samples_per_class:\n",
        "        samples_per_class[label] = img\n",
        "    if len(samples_per_class) == len(class_names):\n",
        "        break\n",
        "wandb.log({\"Fashion-MNIST Sample Images\": [wandb.Image(img, caption=class_names[label]) for label, img in samples_per_class.items()]})\n",
        "\n",
        "# Plot images\n",
        "fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
        "fig.suptitle(\"Fashion-MNIST Sample Images\", fontsize=14)\n",
        "\n",
        "for ax, (label, image) in zip(axes.flat, samples_per_class.items()):\n",
        "    ax.imshow(image, cmap='gray')\n",
        "    ax.set_title(class_names[label])\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#flatten the images\n",
        "train_images = train_images.reshape(train_images.shape[0], -1)\n",
        "test_images = test_images.reshape(test_images.shape[0], -1)"
      ],
      "metadata": {
        "id": "rTX0sCwdgbH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Helper Functions\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Subtract max for numerical stability\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_derivative(z):\n",
        "    return (z > 0).astype(float)\n",
        "\n",
        "\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    n_samples = y_true.shape[0]\n",
        "    logp = -np.log(y_pred[range(n_samples), y_true])\n",
        "    return np.sum(logp) / n_samples\n",
        "# Neural Network Class\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, layer_sizes):\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        self._initialize_parameters()\n",
        "\n",
        "    def _initialize_parameters(self):\n",
        "        \"\"\"Initialize weights and biases for all layers.\"\"\"\n",
        "        for i in range(len(self.layer_sizes) - 1):\n",
        "            fan_in = self.layer_sizes[i]\n",
        "            fan_out = self.layer_sizes[i + 1]\n",
        "            # He initialization for hidden layers, Xavier for output layer\n",
        "            if i < len(self.layer_sizes) - 2:  # Hidden layers use sigmoid\n",
        "                scale = np.sqrt(2 / fan_in)\n",
        "            else:  # Output layer uses softmax\n",
        "                scale = np.sqrt(1 / fan_in)\n",
        "            # Weight matrix: (fan_in, fan_out)\n",
        "            weight = np.random.randn(fan_in, fan_out) * scale\n",
        "            # Bias vector: (fan_out,)\n",
        "            bias = np.zeros(fan_out)\n",
        "            self.weights.append(weight)\n",
        "            self.biases.append(bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        a = x  # Activation starts as the input\n",
        "        # Process hidden layers\n",
        "        for l in range(len(self.weights) - 1):\n",
        "            z = np.dot(a, self.weights[l]) + self.biases[l]\n",
        "            a = sigmoid(z)\n",
        "        # Process output layer\n",
        "        z = np.dot(a, self.weights[-1]) + self.biases[-1]\n",
        "        output = softmax(z)\n",
        "        return output\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Create a network: 784 input -> 256 hidden -> 128 hidden -> 10 output\n",
        "    nn = NeuralNetwork([784, 256, 128, 10])\n",
        "    # Simulate 5 input samples (e.g., Fashion-MNIST images)\n",
        "    x = np.random.randn(5, 784)\n",
        "    # Get probabilities\n",
        "    probs = nn.forward(x)\n",
        "    # Print probabilities for each sample\n",
        "    for i, prob in enumerate(probs):\n",
        "        print(f\"Sample {i+1} probabilities: {prob}\")"
      ],
      "metadata": {
        "id": "7d23rW0dgoX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZxGpz98KhUAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSqUzepQiKKg"
      },
      "outputs": [],
      "source": [
        "def to_one_hot(labels, num_classes=10):\n",
        "    \"\"\"Convert labels to one-hot encoding.\"\"\"\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "# Base Optimizer Class\n",
        "class Optimizer:\n",
        "    \"\"\"Abstract base class for optimizers.\"\"\"\n",
        "    def update(self, weights, biases, grads_w, grads_b):\n",
        "        raise NotImplementedError(\"Subclasses must implement this method.\")\n",
        "\n",
        "# SGD Optimizer\n",
        "class SGD(Optimizer):\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def update(self, weights, biases, grads_w, grads_b):\n",
        "        for i in range(len(weights)):\n",
        "            weights[i] -= self.learning_rate * grads_w[i]\n",
        "            biases[i] -= self.learning_rate * grads_b[i]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIzCahFHjV1t"
      },
      "outputs": [],
      "source": [
        "# Momentum Optimizer\n",
        "class Momentum(Optimizer):\n",
        "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.momentum = momentum\n",
        "        self.velocity_w = None\n",
        "        self.velocity_b = None\n",
        "\n",
        "    def update(self, weights, biases, grads_w, grads_b):\n",
        "        if self.velocity_w is None:\n",
        "            self.velocity_w = [np.zeros_like(w) for w in weights]\n",
        "            self.velocity_b = [np.zeros_like(b) for b in biases]\n",
        "        for i in range(len(weights)):\n",
        "            self.velocity_w[i] = self.momentum * self.velocity_w[i] + self.learning_rate * grads_w[i]\n",
        "            self.velocity_b[i] = self.momentum * self.velocity_b[i] + self.learning_rate * grads_b[i]\n",
        "            weights[i] -= self.velocity_w[i]\n",
        "            biases[i] -= self.velocity_b[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZoZNb2PjdYq"
      },
      "outputs": [],
      "source": [
        "# Nesterov Optimizer\n",
        "class Nesterov(Optimizer):\n",
        "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.momentum = momentum\n",
        "        self.velocity_w = None\n",
        "        self.velocity_b = None\n",
        "\n",
        "    def update(self, weights, biases, grads_w, grads_b):\n",
        "        if self.velocity_w is None:\n",
        "            self.velocity_w = [np.zeros_like(w) for w in weights]\n",
        "            self.velocity_b = [np.zeros_like(b) for b in biases]\n",
        "        for i in range(len(weights)):\n",
        "            # Simplified Nesterov: update with current gradients (approximation)\n",
        "            self.velocity_w[i] = self.momentum * self.velocity_w[i] + self.learning_rate * grads_w[i]\n",
        "            self.velocity_b[i] = self.momentum * self.velocity_b[i] + self.learning_rate * grads_b[i]\n",
        "            weights[i] -= self.momentum * self.velocity_w[i]\n",
        "            biases[i] -= self.momentum * self.velocity_b[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alprFUnmjh4E"
      },
      "outputs": [],
      "source": [
        "# RMSProp Optimizer\n",
        "class RMSProp(Optimizer):\n",
        "    def __init__(self, learning_rate=0.001, beta=0.9, epsilon=1e-8):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta = beta\n",
        "        self.epsilon = epsilon\n",
        "        self.s_w = None\n",
        "        self.s_b = None\n",
        "\n",
        "    def update(self, weights, biases, grads_w, grads_b):\n",
        "        if self.s_w is None:\n",
        "            self.s_w = [np.zeros_like(w) for w in weights]\n",
        "            self.s_b = [np.zeros_like(b) for b in biases]\n",
        "        for i in range(len(weights)):\n",
        "            self.s_w[i] = self.beta * self.s_w[i] + (1 - self.beta) * (grads_w[i] ** 2)\n",
        "            self.s_b[i] = self.beta * self.s_b[i] + (1 - self.beta) * (grads_b[i] ** 2)\n",
        "            weights[i] -= self.learning_rate * grads_w[i] / (np.sqrt(self.s_w[i] + self.epsilon))\n",
        "            biases[i] -= self.learning_rate * grads_b[i] / (np.sqrt(self.s_b[i] + self.epsilon))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4Kn4kYLjo82"
      },
      "outputs": [],
      "source": [
        "# Adam Optimizer\n",
        "class Adam(Optimizer):\n",
        "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.m_w = None\n",
        "        self.m_b = None\n",
        "        self.v_w = None\n",
        "        self.v_b = None\n",
        "        self.t = 0\n",
        "\n",
        "    def update(self, weights, biases, grads_w, grads_b):\n",
        "        if self.m_w is None:\n",
        "            self.m_w = [np.zeros_like(w) for w in weights]\n",
        "            self.m_b = [np.zeros_like(b) for b in biases]\n",
        "            self.v_w = [np.zeros_like(w) for w in weights]\n",
        "            self.v_b = [np.zeros_like(b) for b in biases]\n",
        "        self.t += 1\n",
        "        for i in range(len(weights)):\n",
        "            self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * grads_w[i]\n",
        "            self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * grads_b[i]\n",
        "            self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (grads_w[i] ** 2)\n",
        "            self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (grads_b[i] ** 2)\n",
        "            m_w_hat = self.m_w[i] / (1 - self.beta1 ** self.t)\n",
        "            m_b_hat = self.m_b[i] / (1 - self.beta1 ** self.t)\n",
        "            v_w_hat = self.v_w[i] / (1 - self.beta2 ** self.t)\n",
        "            v_b_hat = self.v_b[i] / (1 - self.beta2 ** self.t)\n",
        "            weights[i] -= self.learning_rate * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
        "            biases[i] -= self.learning_rate * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdENND-1jq6w"
      },
      "outputs": [],
      "source": [
        "# Nadam Optimizer\n",
        "class Nadam(Optimizer):\n",
        "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.m_w = None\n",
        "        self.m_b = None\n",
        "        self.v_w = None\n",
        "        self.v_b = None\n",
        "        self.t = 0\n",
        "\n",
        "    def update(self, weights, biases, grads_w, grads_b):\n",
        "        if self.m_w is None:\n",
        "            self.m_w = [np.zeros_like(w) for w in weights]\n",
        "            self.m_b = [np.zeros_like(b) for b in biases]\n",
        "            self.v_w = [np.zeros_like(w) for w in weights]\n",
        "            self.v_b = [np.zeros_like(b) for b in biases]\n",
        "        self.t += 1\n",
        "        for i in range(len(weights)):\n",
        "            self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * grads_w[i]\n",
        "            self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * grads_b[i]\n",
        "            self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (grads_w[i] ** 2)\n",
        "            self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (grads_b[i] ** 2)\n",
        "            m_w_hat = self.m_w[i] / (1 - self.beta1 ** self.t)\n",
        "            m_b_hat = self.m_b[i] / (1 - self.beta1 ** self.t)\n",
        "            v_w_hat = self.v_w[i] / (1 - self.beta2 ** self.t)\n",
        "            v_b_hat = self.v_b[i] / (1 - self.beta2 ** self.t)\n",
        "            m_w_nesterov = self.beta1 * m_w_hat + (1 - self.beta1) * grads_w[i] / (1 - self.beta1 ** self.t)\n",
        "            m_b_nesterov = self.beta1 * m_b_hat + (1 - self.beta1) * grads_b[i] / (1 - self.beta1 ** self.t)\n",
        "            weights[i] -= self.learning_rate * m_w_nesterov / (np.sqrt(v_w_hat) + self.epsilon)\n",
        "            biases[i] -= self.learning_rate * m_b_nesterov / (np.sqrt(v_b_hat) + self.epsilon)\n"
      ]
    }
  ]
}