{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RigEhsbQfGvs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "import wandb\n",
        "\n",
        "# Load the dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "# Normalize images\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "# Define class names\n",
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
        "#intialize wanddb\n",
        "wandb.init(project=\"deep_learning\",name='mnist_dataset')\n",
        "# Select one image per class\n",
        "samples_per_class = {}\n",
        "for img, label in zip(train_images, train_labels):\n",
        "    if label not in samples_per_class:\n",
        "        samples_per_class[label] = img\n",
        "    if len(samples_per_class) == len(class_names):\n",
        "        break\n",
        "wandb.log({\"Fashion-MNIST Sample Images\": [wandb.Image(img, caption=class_names[label]) for label, img in samples_per_class.items()]})\n",
        "\n",
        "# Plot images\n",
        "fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
        "fig.suptitle(\"Fashion-MNIST Sample Images\", fontsize=14)\n",
        "\n",
        "for ax, (label, image) in zip(axes.flat, samples_per_class.items()):\n",
        "    ax.imshow(image, cmap='gray')\n",
        "    ax.set_title(class_names[label])\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#flatten the images\n",
        "train_images = train_images.reshape(train_images.shape[0], -1)\n",
        "test_images = test_images.reshape(test_images.shape[0], -1)"
      ],
      "metadata": {
        "id": "rTX0sCwdgbH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Helper Functions\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Subtract max for numerical stability\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_derivative(z):\n",
        "    return (z > 0).astype(float)\n",
        "\n",
        "\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    n_samples = y_true.shape[0]\n",
        "    logp = -np.log(y_pred[range(n_samples), y_true])\n",
        "    return np.sum(logp) / n_samples\n",
        "# Neural Network Class\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, layer_sizes):\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        self._initialize_parameters()\n",
        "\n",
        "    def _initialize_parameters(self):\n",
        "        \"\"\"Initialize weights and biases for all layers.\"\"\"\n",
        "        for i in range(len(self.layer_sizes) - 1):\n",
        "            fan_in = self.layer_sizes[i]\n",
        "            fan_out = self.layer_sizes[i + 1]\n",
        "            # He initialization for hidden layers, Xavier for output layer\n",
        "            if i < len(self.layer_sizes) - 2:  # Hidden layers use sigmoid\n",
        "                scale = np.sqrt(2 / fan_in)\n",
        "            else:  # Output layer uses softmax\n",
        "                scale = np.sqrt(1 / fan_in)\n",
        "            # Weight matrix: (fan_in, fan_out)\n",
        "            weight = np.random.randn(fan_in, fan_out) * scale\n",
        "            # Bias vector: (fan_out,)\n",
        "            bias = np.zeros(fan_out)\n",
        "            self.weights.append(weight)\n",
        "            self.biases.append(bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        a = x  # Activation starts as the input\n",
        "        # Process hidden layers\n",
        "        for l in range(len(self.weights) - 1):\n",
        "            z = np.dot(a, self.weights[l]) + self.biases[l]\n",
        "            a = sigmoid(z)\n",
        "        # Process output layer\n",
        "        z = np.dot(a, self.weights[-1]) + self.biases[-1]\n",
        "        output = softmax(z)\n",
        "        return output\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Create a network: 784 input -> 256 hidden -> 128 hidden -> 10 output\n",
        "    nn = NeuralNetwork([784, 256, 128, 10])\n",
        "    # Simulate 5 input samples (e.g., Fashion-MNIST images)\n",
        "    x = np.random.randn(5, 784)\n",
        "    # Get probabilities\n",
        "    probs = nn.forward(x)\n",
        "    # Print probabilities for each sample\n",
        "    for i, prob in enumerate(probs):\n",
        "        print(f\"Sample {i+1} probabilities: {prob}\")"
      ],
      "metadata": {
        "id": "7d23rW0dgoX9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}